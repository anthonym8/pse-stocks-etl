name: Tests

on:
  push:
    branches: [ "main" ]
  pull_request:
    branches: [ "main" ]
  workflow_dispatch:

jobs:

  unit-tests:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout code
      uses: actions/checkout@v3

    - name: Build Docker image
      run: docker build . --file Dockerfile --tag pse-stocks-etl

    - name: Run tests
      run: |
        docker run --name pse-stocks-etl \
          pse-stocks-etl sh -c "pytest tests/unit"


  integration-tests:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout code
      uses: actions/checkout@v3

    - name: Create credentials key file
      env:
        GCP_CREDENTIALS_JSON: '${{ secrets.TEST_GCP_CREDENTIALS_JSON }}'
      run: |
        mkdir credentials 
        printf '%s ' $GCP_CREDENTIALS_JSON >| credentials/keyfile.json;

    - name: Build Docker image
      run: docker build . --file Dockerfile --tag pse-stocks-etl

    - name: Run tests
      env:
        POSTGRES_DB_ENDPOINT: ${{ secrets.TEST_POSTGRES_DB_ENDPOINT }}
        POSTGRES_DB_USERNAME: ${{ secrets.TEST_POSTGRES_DB_USERNAME }}
        POSTGRES_DB_PASSWORD: ${{ secrets.TEST_POSTGRES_DB_PASSWORD }}
        POSTGRES_DB_PORT: ${{ secrets.TEST_POSTGRES_DB_PORT }}
        POSTGRES_DB_NAME: ${{ secrets.TEST_POSTGRES_DB_NAME }}
        GCP_PROJECT_ID: ${{ secrets.TEST_GCP_PROJECT_ID }}
        GCP_CREDENTIALS_FILE: ${{ secrets.TEST_GCP_CREDENTIALS_FILE }}
        GCS_BUCKET_NAME: ${{ secrets.TEST_GCS_BUCKET_NAME }}
        BIGQUERY_LOCATION: ${{ secrets.TEST_BIGQUERY_LOCATION }}
        BIGQUERY_DATASET_ID: ${{ secrets.TEST_BIGQUERY_DATASET_ID }}
      run: |
        docker run --name pse-stocks-etl \
          -e POSTGRES_DB_ENDPOINT=$POSTGRES_DB_ENDPOINT \
          -e POSTGRES_DB_USERNAME=$POSTGRES_DB_USERNAME \
          -e POSTGRES_DB_PASSWORD=$POSTGRES_DB_PASSWORD \
          -e POSTGRES_DB_PORT=$POSTGRES_DB_PORT \
          -e POSTGRES_DB_NAME=$POSTGRES_DB_NAME \
          -e GCP_PROJECT_ID=$GCP_PROJECT_ID \
          -e GCP_CREDENTIALS_FILE=$GCP_CREDENTIALS_FILE \
          -e GCS_BUCKET_NAME=$GCS_BUCKET_NAME \
          -e BIGQUERY_LOCATION=$BIGQUERY_LOCATION \
          -e BIGQUERY_DATASET_ID=$BIGQUERY_DATASET_ID \
          -v $PWD/credentials:/pse-stocks-etl/credentials \
          pse-stocks-etl sh -c "pytest tests/integration"


  e2e-test-run-postgres:
    runs-on: ubuntu-latest

    steps:
    
    - name: Checkout code
      uses: actions/checkout@v3

    - name: Build Docker image
      run: docker build . --file Dockerfile --tag pse-stocks-etl
          
    - name: Run sync job
      env:
        POSTGRES_DB_ENDPOINT: ${{ secrets.TEST_POSTGRES_DB_ENDPOINT }}
        POSTGRES_DB_USERNAME: ${{ secrets.TEST_POSTGRES_DB_USERNAME }}
        POSTGRES_DB_PASSWORD: ${{ secrets.TEST_POSTGRES_DB_PASSWORD }}
        POSTGRES_DB_PORT: ${{ secrets.TEST_POSTGRES_DB_PORT }}
        POSTGRES_DB_NAME: ${{ secrets.TEST_POSTGRES_DB_NAME }}
      run: |
        docker run --name pse-stocks-etl \
          -e POSTGRES_DB_ENDPOINT=$POSTGRES_DB_ENDPOINT \
          -e POSTGRES_DB_USERNAME=$POSTGRES_DB_USERNAME \
          -e POSTGRES_DB_PASSWORD=$POSTGRES_DB_PASSWORD \
          -e POSTGRES_DB_PORT=$POSTGRES_DB_PORT \
          -e POSTGRES_DB_NAME=$POSTGRES_DB_NAME \
          pse-stocks-etl python -m src.main \
          --destination postgres \
          --action sync \
          --concurrency=1


  e2e-test-run-bigquery:
    runs-on: ubuntu-latest

    steps:
    
    - name: Checkout code
      uses: actions/checkout@v3

    - name: Create credentials key file
      env:
        GCP_CREDENTIALS_JSON: '${{ secrets.TEST_GCP_CREDENTIALS_JSON }}'
      run: |
        mkdir credentials 
        printf '%s ' $GCP_CREDENTIALS_JSON >| credentials/keyfile.json;

    - name: Build Docker image
      run: docker build . --file Dockerfile --tag pse-stocks-etl
          
    - name: Run sync job
      env:
        GCP_PROJECT_ID: ${{ secrets.TEST_GCP_PROJECT_ID }}
        GCP_CREDENTIALS_FILE: ${{ secrets.TEST_GCP_CREDENTIALS_FILE }}
        GCS_BUCKET_NAME: ${{ secrets.TEST_GCS_BUCKET_NAME }}
        BIGQUERY_LOCATION: ${{ secrets.TEST_BIGQUERY_LOCATION }}
        BIGQUERY_DATASET_ID: ${{ secrets.TEST_BIGQUERY_DATASET_ID }}
      run: |
        docker run --name pse-stocks-etl \
          -e GCP_PROJECT_ID=$GCP_PROJECT_ID \
          -e GCP_CREDENTIALS_FILE=$GCP_CREDENTIALS_FILE \
          -e GCS_BUCKET_NAME=$GCS_BUCKET_NAME \
          -e BIGQUERY_LOCATION=$BIGQUERY_LOCATION \
          -e BIGQUERY_DATASET_ID=$BIGQUERY_DATASET_ID \
          -v $PWD/credentials:/pse-stocks-etl/credentials \
          pse-stocks-etl python -m src.main \
          --destination bigquery \
          --action sync \
          --concurrency=1
          

  e2e-test-run-deltalake:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout code
      uses: actions/checkout@v3

    - name: Create credentials key file
      env:
        GCP_CREDENTIALS_JSON: '${{ secrets.TEST_GCP_CREDENTIALS_JSON }}'
      run: |
        mkdir credentials 
        printf '%s ' $GCP_CREDENTIALS_JSON >| credentials/keyfile.json;

    - name: Build Docker image
      run: docker build . --file Dockerfile --tag pse-stocks-etl

    - name: Run sync job
      env:
        GCP_PROJECT_ID: ${{ secrets.TEST_GCP_PROJECT_ID }}
        GCP_CREDENTIALS_FILE: ${{ secrets.TEST_GCP_CREDENTIALS_FILE }}
        GCS_BUCKET_NAME: ${{ secrets.TEST_GCS_BUCKET_NAME }}
      run: |
        source version.env
        echo "IMAGE_VERSION=$IMAGE_VERSION"
        docker run \
          -e GCP_PROJECT_ID=$GCP_PROJECT_ID \
          -e GCP_CREDENTIALS_FILE=$GCP_CREDENTIALS_FILE \
          -e GCS_BUCKET_NAME=$GCS_BUCKET_NAME \
          -v $PWD/credentials:/pse-stocks-etl/credentials \
          pse-stocks-etl \
          python -m src.main \
          --destination deltalake \
          --action sync \
          --concurrency=1
          

  e2e-test-run-spark-deltalake:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout code
      uses: actions/checkout@v3

    - name: Create credentials key file
      env:
        GCP_CREDENTIALS_JSON: '${{ secrets.TEST_GCP_CREDENTIALS_JSON }}'
      run: |
        mkdir credentials 
        printf '%s ' $GCP_CREDENTIALS_JSON >| credentials/keyfile.json;

    - name: Build Docker image
      run: docker build . --file src/db/delta_spark/Dockerfile-spark --tag pse-stocks-etl-spark

    - name: Run sync job
      env:
        GCP_CREDENTIALS_FILE: ${{ secrets.TEST_GCP_CREDENTIALS_FILE }}
        DELTA_TABLE_PATH_PREFIX: ${{ secrets.TEST_DELTA_TABLE_PATH_PREFIX }}
      run: |
        docker run \
          -e GCP_CREDENTIALS_FILE=$GCP_CREDENTIALS_FILE \
          -e DELTA_TABLE_PATH_PREFIX=$DELTA_TABLE_PATH_PREFIX \
          -v $PWD/credentials:/home/glue_user/pse-stocks-etl/credentials \
          pse-stocks-etl-spark